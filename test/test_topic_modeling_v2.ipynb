{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|▋         | 1/15 [03:11<44:42, 191.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.07\n",
      "Epoch 2: Average Training Loss: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  13%|█▎        | 2/15 [06:45<44:23, 204.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 3: Average Training Loss: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 3/15 [09:56<39:43, 198.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 4: Average Training Loss: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  27%|██▋       | 4/15 [13:10<36:04, 196.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 5: Average Training Loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 5/15 [16:32<33:06, 198.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 6: Average Training Loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 6/15 [19:51<29:47, 198.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 7: Average Training Loss: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  47%|████▋     | 7/15 [23:07<26:22, 197.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 8: Average Training Loss: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  53%|█████▎    | 8/15 [26:10<22:31, 193.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 9: Average Training Loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 9/15 [29:06<18:45, 187.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 10: Average Training Loss: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 10/15 [32:01<15:19, 183.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 11: Average Training Loss: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  73%|███████▎  | 11/15 [35:02<12:11, 182.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 12: Average Training Loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 12/15 [38:16<09:19, 186.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 13: Average Training Loss: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  87%|████████▋ | 13/15 [41:16<06:08, 184.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"midas/duc2001\", \"raw\")[\"test\"]\n",
    "dataset = load_dataset(\"midas/inspec\", \"raw\")[\"test\"]\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Constants\n",
    "MAX_LEN = 75\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Prepare mapping for labels\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "\n",
    "# Adjust these weights based on your specific dataset and class imbalance\n",
    "class_weights = torch.tensor([10.0, 15.0, 0.1])  # Example weights for 'B', 'I', 'O'\n",
    "# class_weights = torch.tensor([10.0, 15.0, 0.1]).cuda()  # Example weights for 'B', 'I', 'O' if GPU applicable\n",
    "\n",
    "# Tokenization and encoding for BERT\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    # Join tokens into a single string\n",
    "    text = ' '.join([t.lower() for t in item['document']])\n",
    "    tags = item['doc_bio_tags']\n",
    "\n",
    "    # Encode text\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Prepare labels\n",
    "    tag_ids = [tag2idx[tag] for tag in tags] + [tag2idx['O']] * (MAX_LEN - len(tags))\n",
    "    tag_ids = tag_ids[:MAX_LEN]  # Ensure label length matches input length\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'][0])\n",
    "    attention_masks.append(encoded_dict['attention_mask'][0])\n",
    "    labels.append(torch.tensor(tag_ids))\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.stack(input_ids)\n",
    "attention_masks = torch.stack(attention_masks)\n",
    "labels = torch.stack(labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels, train_masks, val_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks, test_size=0.1, random_state=2018\n",
    ")\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load BERT for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, eps=1e-8)  # increased learning rate\n",
    "\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 4)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# We have a class imbalance which is hindering our model performance\n",
    "# Apply focal loss to focus more on hard-to-classify examples by down-weighting the loss contributed by well-classified examples(easy-classify)\n",
    "def hybrid_loss(logits, labels, weights, alpha=0.8, gamma=2.0):\n",
    "    # Softmax and cross entropy loss\n",
    "    ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction='none', weight=weights)\n",
    "    \n",
    "    # Calculate probabilities of the true class\n",
    "    p_t = torch.exp(-ce_loss)\n",
    "    \n",
    "    # Calculate focal component\n",
    "    focal_loss = (alpha * (1 - p_t) ** gamma * ce_loss).mean()\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(15), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = hybrid_loss(outputs.logits.view(-1, 3), b_labels.view(-1), class_weights)\n",
    "\n",
    "        # # Apply class weights\n",
    "        # log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "        # weighted_loss = torch.nn.functional.nll_loss(log_probs.view(-1, model.num_labels), b_labels.view(-1), weight=class_weights)\n",
    "\n",
    "        # weighted_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # total_loss += weighted_loss.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_dataloader):.2f}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    print(f'Validation Accuracy: {eval_accuracy / nb_eval_steps:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.06064516129032258\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the tokenizer\n",
    "model.save_pretrained('./model_save_v3/')\n",
    "tokenizer.save_pretrained('./model_save_v3/')\n",
    "\n",
    "# Load the model and the tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./model_save_v3/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./model_save_v3/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(text, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Document to encode.\n",
    "        add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,             # Pad or truncate.\n",
    "        padding='max_length',      # Pad to max_length.\n",
    "        truncation=True,           # Truncate to max_length.\n",
    "        return_attention_mask=True,# Construct attention masks.\n",
    "        return_tensors='pt',       # Return PyTorch tensors.\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the correct device\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Decode predictions\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = predictions[0].tolist()  # Remove the batch dimension and convert to list\n",
    "\n",
    "    # Convert input_ids to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # print(\"Tokens and Predictions:\")  # Debugging output\n",
    "    # for token, prediction in zip(tokens, predictions):\n",
    "    #     print(f\"{token}: {prediction}\")\n",
    "\n",
    "    # Extract keywords based on the 'B' and 'I' predictions\n",
    "    keywords = []\n",
    "    current_keyword = []\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        if pred == 1:  # Corresponds to 'B'\n",
    "            if current_keyword:  # Save the previous keyword if it exists\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "            current_keyword = [token]  # Start a new keyword\n",
    "        elif pred == 2 and current_keyword:  # Corresponds to 'I'\n",
    "            current_keyword.append(token)\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "                current_keyword = []\n",
    "\n",
    "    # Check if the last token was part of a keyword\n",
    "    if current_keyword:\n",
    "        keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['machine', 'learning', 'artificial', 'statistical', 'algorithms', 'learn', 'data', 'data', 'thus', 'tasks', 'explicit', 'instructions', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different dataset.\n",
    "optimizing loss function -> according to diff feature\n",
    "                            \n",
    "\n",
    "augmenting the weight to each class\n",
    "\n",
    "add dropout layer => \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
