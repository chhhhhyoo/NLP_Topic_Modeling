{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'doc_bio_tags', 'extractive_keyphrases', 'abstractive_keyphrases', 'other_metadata'],\n",
       "    num_rows: 308\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duc2001_dataset = load_dataset(\"midas/duc2001\", \"raw\")[\"test\"]\n",
    "duc2001_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [05:53<17:39, 353.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 2: Average Training Loss: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [11:59<12:02, 361.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 3: Average Training Loss: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [18:27<06:13, 373.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n",
      "Epoch 4: Average Training Loss: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [24:36<00:00, 369.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load datasets\n",
    "duc2001_dataset = load_dataset(\"midas/duc2001\", \"raw\")[\"test\"]\n",
    "# inspec_dataset = load_dataset(\"midas/inspec\", \"raw\")[\"test\"]\n",
    "# nus_dataset = load_dataset(\"midas/nus\", \"raw\")[\"test\"]\n",
    "\n",
    "def dataset_to_dataframe(dataset):\n",
    "    return pd.DataFrame({\n",
    "        'document': [item['document'] for item in dataset],\n",
    "        'doc_bio_tags': [item['doc_bio_tags'] for item in dataset]\n",
    "    })\n",
    "\n",
    "# Convert datasets to dataframes\n",
    "duc2001_df = dataset_to_dataframe(duc2001_dataset)\n",
    "inspec_df = dataset_to_dataframe(inspec_dataset)\n",
    "nus_df = dataset_to_dataframe(nus_dataset)\n",
    "\n",
    "# Concatenate dataframes\n",
    "combined_df = pd.concat([duc2001_df, inspec_df, nus_df], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame back to a Hugging Face dataset\n",
    "combined_dataset = Dataset.from_pandas(combined_df)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Constants\n",
    "MAX_LEN = 75\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Prepare mapping for labels\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "\n",
    "# Adjust these weights based on your specific dataset and class imbalance\n",
    "class_weights = torch.tensor([10.0, 15.0, 0.1])  # Example weights for 'B', 'I', 'O'\n",
    "# class_weights = torch.tensor([10.0, 15.0, 0.1]).cuda()  # Example weights for 'B', 'I', 'O' if GPU applicable\n",
    "\n",
    "# Tokenization and encoding for BERT\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for i, item in enumerate(combined_dataset):\n",
    "    # Join tokens into a single string\n",
    "    text = ' '.join([t.lower() for t in item['document']])\n",
    "    tags = item['doc_bio_tags']\n",
    "\n",
    "    # Encode text\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Prepare labels\n",
    "    tag_ids = [tag2idx[tag] for tag in tags] + [tag2idx['O']] * (MAX_LEN - len(tags))\n",
    "    tag_ids = tag_ids[:MAX_LEN]  # Ensure label length matches input length\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'][0])\n",
    "    attention_masks.append(encoded_dict['attention_mask'][0])\n",
    "    labels.append(torch.tensor(tag_ids))\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.stack(input_ids)\n",
    "attention_masks = torch.stack(attention_masks)\n",
    "labels = torch.stack(labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels, train_masks, val_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks, test_size=0.1, random_state=2018\n",
    ")\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load BERT for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, eps=1e-8)  # increased learning rate\n",
    "\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 4)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# We have a class imbalance which is hindering our model performance\n",
    "# Apply focal loss to focus more on hard-to-classify examples by down-weighting the loss contributed by well-classified examples(easy-classify)\n",
    "def hybrid_loss(logits, labels, weights, alpha=0.8, gamma=2.0):\n",
    "    # Softmax and cross entropy loss\n",
    "    ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction='none', weight=weights)\n",
    "    \n",
    "    # Calculate probabilities of the true class\n",
    "    p_t = torch.exp(-ce_loss)\n",
    "    \n",
    "    # Calculate focal component\n",
    "    focal_loss = (alpha * (1 - p_t) ** gamma * ce_loss).mean()\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(4), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = hybrid_loss(outputs.logits.view(-1, 3), b_labels.view(-1), class_weights)\n",
    "\n",
    "        # # Apply class weights\n",
    "        # log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "        # weighted_loss = torch.nn.functional.nll_loss(log_probs.view(-1, model.num_labels), b_labels.view(-1), weight=class_weights)\n",
    "\n",
    "        # weighted_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # total_loss += weighted_loss.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_dataloader):.2f}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    print(f'Validation Accuracy: {eval_accuracy / nb_eval_steps:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.08\n",
      "Validation Recall: 0.65\n",
      "Validation F1 Score: 0.15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_metrics(preds, labels, metrics):\n",
    "    preds_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    tp = np.sum((preds_flat == labels_flat) & (labels_flat != tag2idx['O']))\n",
    "    fp = np.sum((preds_flat != labels_flat) & (preds_flat != tag2idx['O']))\n",
    "    fn = np.sum((preds_flat != labels_flat) & (labels_flat != tag2idx['O']))\n",
    "\n",
    "    metrics['tp'] += tp\n",
    "    metrics['fp'] += fp\n",
    "    metrics['fn'] += fn\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_scores(metrics):\n",
    "    precision = metrics['tp'] / (metrics['tp'] + metrics['fp']) if metrics['tp'] + metrics['fp'] > 0 else 0\n",
    "    recall = metrics['tp'] / (metrics['tp'] + metrics['fn']) if metrics['tp'] + metrics['fn'] > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Modified validation loop\n",
    "def validate_model(valid_dataloader, model):\n",
    "    model.eval()\n",
    "    eval_metrics = {'tp': 0, 'fp': 0, 'fn': 0}\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_metrics = update_metrics(logits, label_ids, eval_metrics)\n",
    "\n",
    "    precision, recall, f1 = calculate_scores(eval_metrics)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Example of calling the validate_model function\n",
    "precision, recall, f1 = validate_model(valid_dataloader, model)\n",
    "print(f\"Validation Precision: {precision:.2f}\")\n",
    "print(f\"Validation Recall: {recall:.2f}\")\n",
    "print(f\"Validation F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.08131944444444444\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the tokenizer\n",
    "model.save_pretrained('./model_save_v5/')\n",
    "tokenizer.save_pretrained('./model_save_v5/')\n",
    "\n",
    "# Load the model and the tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./model_save_v5/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./model_save_v5/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(text, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Document to encode.\n",
    "        add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,             # Pad or truncate.\n",
    "        padding='max_length',      # Pad to max_length.\n",
    "        truncation=True,           # Truncate to max_length.\n",
    "        return_attention_mask=True,# Construct attention masks.\n",
    "        return_tensors='pt',       # Return PyTorch tensors.\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the correct device\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Decode predictions\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = predictions[0].tolist()  # Remove the batch dimension and convert to list\n",
    "\n",
    "    # Convert input_ids to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # print(\"Tokens and Predictions:\")  # Debugging output\n",
    "    # for token, prediction in zip(tokens, predictions):\n",
    "    #     print(f\"{token}: {prediction}\")\n",
    "\n",
    "    # Extract keywords based on the 'B' and 'I' predictions\n",
    "    keywords = []\n",
    "    current_keyword = []\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        if pred == 1:  # Corresponds to 'B'\n",
    "            if current_keyword:  # Save the previous keyword if it exists\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "            current_keyword = [token]  # Start a new keyword\n",
    "        elif pred == 2 and current_keyword:  # Corresponds to 'I'\n",
    "            current_keyword.append(token)\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "                current_keyword = []\n",
    "    \n",
    "    # Check if the last token was part of a keyword\n",
    "    if current_keyword:\n",
    "        keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['machine', 'learning', 'ml', ')', 'field', 'artificial', 'intelligence', 'development', 'study', 'of', 'statistical', 'algorithms', 'ize', 'thus', 'perform', 'without', 'explicit', 'instructions', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['machine', 'learning', 'ml', 'artificial', 'intelligence', 'statistical', 'algorithms', 'learn', 'data', 'explicit', 'instructions']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['machine', 'learning', '(', 'ml', ')', 'field', 'in', 'artificial', 'intelligence', 'with', 'development', 'study', 'of', 'statistical', 'algorithms', 'data', 'and', 'ize', 'to', 'data', ',', 'and', 'thus', 'perform', 'tasks', 'without', 'instructions', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['machine', 'learning', 'ml', 'artificial', 'intelligence', 'statistical', 'algorithms', 'learn', 'data', 'explicit', 'instructions']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different dataset.\n",
    "optimizing loss function -> according to diff feature\n",
    "                            \n",
    "\n",
    "augmenting the weight to each class\n",
    "\n",
    "add dropout layer => 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(relevance_scores, k, method=1):\n",
    "    \"\"\"Calculate discounted cumulative gain (DCG) at rank k.\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (list of float): The list of relevance scores.\n",
    "        k (int): The number of results to consider.\n",
    "        method (int): The method to compute DCG, 0 or 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The DCG score.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the method is not 0 or 1.\n",
    "    \"\"\"\n",
    "    relevance_scores = np.asfarray(relevance_scores)[:k]\n",
    "    if relevance_scores.size:\n",
    "        if method == 0:\n",
    "            return relevance_scores[0] + np.sum(relevance_scores[1:] / np.log2(np.arange(2, relevance_scores.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(relevance_scores, k, method=1):\n",
    "    \"\"\"Calculate normalized discounted cumulative gain (NDCG) at rank k.\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (list of float): The list of relevance scores.\n",
    "        k (int): The number of results to consider.\n",
    "        method (int): The method to compute DCG, 0 or 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The NDCG score.\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(relevance_scores, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.0\n",
    "    return dcg_at_k(relevance_scores, k, method) / dcg_max\n",
    "\n",
    "def mean_reciprocal_rank(ranking_lists):\n",
    "    \"\"\"Calculate the mean reciprocal rank (MRR).\n",
    "\n",
    "    Args:\n",
    "        ranking_lists (list of list of int): Each inner list is a set of binary values (0 or 1)\n",
    "            indicating the absence or presence of relevant items.\n",
    "\n",
    "    Returns:\n",
    "        float: The MRR score.\n",
    "    \"\"\"\n",
    "    first_relevant = (np.asarray(rankings).nonzero()[0] for rankings in ranking_lists)\n",
    "    return np.mean([1.0 / (ranking[0] + 1) if ranking.size else 0 for ranking in first_relevant])\n",
    "\n",
    "def calculate_relevance_scores(true_keywords, predicted_keywords):\n",
    "    \"\"\"Calculates relevance scores where 1 indicates relevance and 0 indicates irrelevance.\n",
    "   \n",
    "    Args:\n",
    "        true_keywords (list of str): The list of true keywords.\n",
    "        predicted_keywords (list of tuples): List of predicted keywords with their scores.\n",
    "   \n",
    "    Returns:\n",
    "        list of int: Relevance scores (1 or 0) for each predicted keyword.\n",
    "    \"\"\"\n",
    "    return [1 if keyword in true_keywords else 0 for keyword, _ in predicted_keywords]\n",
    "\n",
    "def evaluate_keyword_extraction(true_data, predictions):\n",
    "    \"\"\"Evaluates the keyword extraction algorithm using NDCG and MRR scoring metrics.\n",
    "   \n",
    "    Args:\n",
    "        true_data (list of list of str): List of lists containing true keywords for each document.\n",
    "        predictions (list of list of tuples): List of lists, each containing tuples of keywords and their confidence scores.\n",
    "   \n",
    "    Returns:\n",
    "        tuple of (float, float): Mean NDCG score and Mean MRR score.\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    for true_keywords, predicted_keywords_with_scores in zip(true_data, predictions):\n",
    "        predicted_keywords_with_scores.sort(key=lambda x: x[1], reverse=True)  # Sort by confidence score descending\n",
    "        predicted_keywords = [kw for kw, _ in predicted_keywords_with_scores]\n",
    "        relevance_scores = calculate_relevance_scores(true_keywords, predicted_keywords_with_scores)\n",
    "\n",
    "        print(\"Predicted Keywords with Scores After Sorting:\", predicted_keywords_with_scores)\n",
    "\n",
    "\n",
    "        print(\"True Keywords:\", true_keywords)\n",
    "        print(\"Predicted Keywords:\", predicted_keywords)\n",
    "        print(\"Relevance Scores:\", relevance_scores)\n",
    "\n",
    "        # Compute NDCG\n",
    "        ndcg_score = ndcg_at_k(relevance_scores, k=len(relevance_scores))\n",
    "        ndcg_scores.append(ndcg_score)\n",
    "       \n",
    "        # Compute MRR\n",
    "        rs = [[1 if keyword in true_keywords else 0 for keyword in predicted_keywords]]\n",
    "        mrr_score = mean_reciprocal_rank(rs)\n",
    "        mrr_scores.append(mrr_score)\n",
    "   \n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "    mean_mrr = np.mean(mrr_scores)\n",
    "    return mean_ndcg, mean_mrr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(text, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Document to encode.\n",
    "        add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,             # Pad or truncate.\n",
    "        padding='max_length',      # Pad to max_length.\n",
    "        truncation=True,           # Truncate to max_length.\n",
    "        return_attention_mask=True,# Construct attention masks.\n",
    "        return_tensors='pt',       # Return PyTorch tensors.\n",
    "    )\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = predictions[0].tolist()  # Remove the batch dimension\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Extract keywords based on predictions\n",
    "    keywords = []\n",
    "    current_keyword = \"\"\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        # If the token is part of a keyword\n",
    "        if pred in [1, 2]:\n",
    "            # Remove the BERT's subword prefix if applicable\n",
    "            if token.startswith(\"##\"):\n",
    "                token = token[2:]\n",
    "            current_keyword += token\n",
    "        # If the token marks the end of a keyword\n",
    "        elif current_keyword:\n",
    "            keywords.append(current_keyword)\n",
    "            current_keyword = \"\"\n",
    "    \n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(relevance_scores, k, method=1):\n",
    "    \"\"\"Calculate discounted cumulative gain (DCG) at rank k.\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (list of float): The list of relevance scores.\n",
    "        k (int): The number of results to consider.\n",
    "        method (int): The method to compute DCG, 0 or 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The DCG score.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the method is not 0 or 1.\n",
    "    \"\"\"\n",
    "    relevance_scores = np.asfarray(relevance_scores)[:k]\n",
    "    if relevance_scores.size:\n",
    "        if method == 0:\n",
    "            return relevance_scores[0] + np.sum(relevance_scores[1:] / np.log2(np.arange(2, relevance_scores.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(relevance_scores, k, method=1):\n",
    "    \"\"\"Calculate normalized discounted cumulative gain (NDCG) at rank k.\n",
    "\n",
    "    Args:\n",
    "        relevance_scores (list of float): The list of relevance scores.\n",
    "        k (int): The number of results to consider.\n",
    "        method (int): The method to compute DCG, 0 or 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The NDCG score.\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(relevance_scores, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.0\n",
    "    return dcg_at_k(relevance_scores, k, method) / dcg_max\n",
    "\n",
    "def mean_reciprocal_rank(ranking_lists):\n",
    "    \"\"\"Calculate the mean reciprocal rank (MRR).\n",
    "\n",
    "    Args:\n",
    "        ranking_lists (list of list of int): Each inner list is a set of binary values (0 or 1)\n",
    "            indicating the absence or presence of relevant items.\n",
    "\n",
    "    Returns:\n",
    "        float: The MRR score.\n",
    "    \"\"\"\n",
    "    first_relevant = (np.asarray(rankings).nonzero()[0] for rankings in ranking_lists)\n",
    "    return np.mean([1.0 / (ranking[0] + 1) if ranking.size else 0 for ranking in first_relevant])\n",
    "\n",
    "def calculate_relevance_scores(true_keywords, predicted_keywords):\n",
    "    \"\"\"Calculates relevance scores where 1 indicates relevance and 0 indicates irrelevance.\n",
    "   \n",
    "    Args:\n",
    "        true_keywords (list of str): The list of true keywords.\n",
    "        predicted_keywords (list of tuples): List of predicted keywords with their scores.\n",
    "   \n",
    "    Returns:\n",
    "        list of int: Relevance scores (1 or 0) for each predicted keyword.\n",
    "    \"\"\"\n",
    "    return [1 if keyword in true_keywords else 0 for keyword, _ in predicted_keywords]\n",
    "\n",
    "def evaluate_keyword_extraction(true_data, predictions):\n",
    "    \"\"\"Evaluates the keyword extraction algorithm using NDCG and MRR scoring metrics.\n",
    "   \n",
    "    Args:\n",
    "        true_data (list of list of str): List of lists containing true keywords for each document.\n",
    "        predictions (list of list of tuples): List of lists, each containing tuples of keywords and their confidence scores.\n",
    "   \n",
    "    Returns:\n",
    "        tuple of (float, float): Mean NDCG score and Mean MRR score.\n",
    "    \"\"\"\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "\n",
    "    for true_keywords, predicted_keywords_with_scores in zip(true_data, predictions):\n",
    "        predicted_keywords_with_scores.sort(key=lambda x: x[1], reverse=True)  # Sort by confidence score descending\n",
    "        predicted_keywords = [kw for kw, _ in predicted_keywords_with_scores]\n",
    "        relevance_scores = calculate_relevance_scores(true_keywords, predicted_keywords_with_scores)\n",
    "\n",
    "        # Compute NDCG\n",
    "        ndcg_score = ndcg_at_k(relevance_scores, k=len(relevance_scores))\n",
    "        ndcg_scores.append(ndcg_score)\n",
    "       \n",
    "        # Compute MRR\n",
    "        rs = [[1 if keyword in true_keywords else 0 for keyword in predicted_keywords]]\n",
    "        mrr_score = mean_reciprocal_rank(rs)\n",
    "        mrr_scores.append(mrr_score)\n",
    "   \n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "    mean_mrr = np.mean(mrr_scores)\n",
    "    return mean_ndcg, mean_mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(text, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Document to encode.\n",
    "        add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,             # Pad or truncate.\n",
    "        padding='max_length',      # Pad to max_length.\n",
    "        truncation=True,           # Truncate to max_length.\n",
    "        return_attention_mask=True,# Construct attention masks.\n",
    "        return_tensors='pt',       # Return PyTorch tensors.\n",
    "    )\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = predictions[0].tolist()  # Remove the batch dimension\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Extract keywords based on predictions\n",
    "    keywords = []\n",
    "    current_keyword = []\n",
    "    keyword_scores = []\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        # Remove the BERT's subword prefix if applicable\n",
    "        if token.startswith(\"##\"):\n",
    "            token = token[2:]\n",
    "        else:\n",
    "            # Append and reset the current keyword when encountering a new starting token without '##'\n",
    "            if current_keyword:\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "                keyword_scores.append(max(keyword_scores))\n",
    "                current_keyword = []\n",
    "                keyword_scores = []\n",
    "\n",
    "        # Calculate confidence\n",
    "        confidence = torch.softmax(logits, dim=-1)[0, :, pred].max().item()\n",
    "\n",
    "        if pred == 1:  # 'B' for beginning of keyword\n",
    "            if current_keyword:\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "                keyword_scores.append(max(keyword_scores))\n",
    "            current_keyword = [token]\n",
    "            keyword_scores = [confidence]\n",
    "        elif pred == 2 and current_keyword:  # 'I' for continuation\n",
    "            current_keyword.append(token)\n",
    "            keyword_scores.append(confidence)\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "                keyword_scores.append(max(keyword_scores))\n",
    "                current_keyword = []\n",
    "                keyword_scores = []\n",
    "\n",
    "    if current_keyword:\n",
    "        keywords.append(\"\".join(current_keyword).replace(\"##\", \"\"))\n",
    "        keyword_scores.append(max(keyword_scores))\n",
    "\n",
    "    return list(zip(keywords, keyword_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(text, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Document to encode.\n",
    "        add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,             # Pad or truncate.\n",
    "        padding='max_length',      # Pad to max_length.\n",
    "        truncation=True,           # Truncate to max_length.\n",
    "        return_attention_mask=True,# Construct attention masks.\n",
    "        return_tensors='pt',       # Return PyTorch tensors.\n",
    "    )\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    predictions = predictions[0].tolist()  # Remove the batch dimension\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Extract keywords based on predictions\n",
    "    keywords = []\n",
    "    current_keyword = \"\"\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        # If the token is part of a keyword\n",
    "        if pred in [1, 2]:\n",
    "            # Remove the BERT's subword prefix if applicable\n",
    "            if token.startswith(\"##\"):\n",
    "                token = token[2:]\n",
    "            current_keyword += token\n",
    "        # If the token marks the end of a keyword\n",
    "        elif current_keyword:\n",
    "            keywords.append(current_keyword)\n",
    "            current_keyword = \"\"\n",
    "    \n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m predicted_keywords_with_scores_list \u001b[38;5;241m=\u001b[39m [predicted_keywords_with_scores]  \u001b[38;5;66;03m# Note the double brackets\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m mean_ndcg, mean_mrr \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_keyword_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_keywords_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_keywords_with_scores_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean NDCG: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_ndcg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean MRR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_mrr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 80\u001b[0m, in \u001b[0;36mevaluate_keyword_extraction\u001b[1;34m(true_data, predictions)\u001b[0m\n\u001b[0;32m     77\u001b[0m mrr_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true_keywords, predicted_keywords_with_scores \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(true_data, predictions):\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mpredicted_keywords_with_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Sort by confidence score descending\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     predicted_keywords \u001b[38;5;241m=\u001b[39m [kw \u001b[38;5;28;01mfor\u001b[39;00m kw, _ \u001b[38;5;129;01min\u001b[39;00m predicted_keywords_with_scores]\n\u001b[0;32m     82\u001b[0m     relevance_scores \u001b[38;5;241m=\u001b[39m calculate_relevance_scores(true_keywords, predicted_keywords_with_scores)\n",
      "Cell \u001b[1;32mIn[37], line 80\u001b[0m, in \u001b[0;36mevaluate_keyword_extraction.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     77\u001b[0m mrr_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true_keywords, predicted_keywords_with_scores \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(true_data, predictions):\n\u001b[1;32m---> 80\u001b[0m     predicted_keywords_with_scores\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Sort by confidence score descending\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     predicted_keywords \u001b[38;5;241m=\u001b[39m [kw \u001b[38;5;28;01mfor\u001b[39;00m kw, _ \u001b[38;5;129;01min\u001b[39;00m predicted_keywords_with_scores]\n\u001b[0;32m     82\u001b[0m     relevance_scores \u001b[38;5;241m=\u001b[39m calculate_relevance_scores(true_keywords, predicted_keywords_with_scores)\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# Prepare data for evaluation\n",
    "true_keywords_list = [['machine', 'learning', 'ml', 'artificial', 'intelligence', 'statistical', 'algorithms', 'learn', 'data', 'explicit', 'instructions']]  # Note the double brackets\n",
    "\n",
    "text = \"\"\"Machine learning (ML) is a field of study in artificial intelligence \n",
    "concerned with the development and study of statistical algorithms that \n",
    "can learn from data and generalize to unseen data, and thus \n",
    "perform tasks without explicit instructions.\"\"\"\n",
    "\n",
    "predicted_keywords_with_scores = keywordextract(text, model, tokenizer, device)\n",
    "predicted_keywords_with_scores_list = [predicted_keywords_with_scores]  # Note the double brackets\n",
    "\n",
    "# Evaluation\n",
    "mean_ndcg, mean_mrr = evaluate_keyword_extraction(true_keywords_list, predicted_keywords_with_scores_list)\n",
    "print(f\"Mean NDCG: {mean_ndcg:.3f}\")\n",
    "print(f\"Mean MRR: {mean_mrr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['[CLS]', 'machine', 'ml', 'development', 'study', 'of', 'statistical', 'data', 'general', 'ize', 'unseen', 'data', 'thus', 'perform', 'tasks', 'explicit', 'instructions', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m true_keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martificial\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintelligence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatistical\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate relevance scores\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m relevance_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_relevance_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_keywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_keywords_with_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate keyword extraction\u001b[39;00m\n\u001b[0;32m     27\u001b[0m mean_ndcg, mean_mrr \u001b[38;5;241m=\u001b[39m evaluate_keyword_extraction([true_keywords], [predicted_keywords_with_scores])\n",
      "Cell \u001b[1;32mIn[37], line 64\u001b[0m, in \u001b[0;36mcalculate_relevance_scores\u001b[1;34m(true_keywords, predicted_keywords)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_relevance_scores\u001b[39m(true_keywords, predicted_keywords):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates relevance scores where 1 indicates relevance and 0 indicates irrelevance.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m   \u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        list of int: Relevance scores (1 or 0) for each predicted keyword.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m true_keywords \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m keyword, _ \u001b[38;5;129;01min\u001b[39;00m predicted_keywords]\n",
      "Cell \u001b[1;32mIn[37], line 64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_relevance_scores\u001b[39m(true_keywords, predicted_keywords):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates relevance scores where 1 indicates relevance and 0 indicates irrelevance.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m   \u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        list of int: Relevance scores (1 or 0) for each predicted keyword.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m true_keywords \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m keyword, _ \u001b[38;5;129;01min\u001b[39;00m predicted_keywords]\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the text\n",
    "text = \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions\"\n",
    "\n",
    "# Predict keywords\n",
    "predicted_keywords_with_scores = ['machine', 'learning', 'ml', 'artificial', 'intelligence', 'statistical', 'algorithms', 'learn', 'data', 'explicit', 'instructions']\n",
    "\n",
    "# Define the true keywords\n",
    "true_keywords = ['machine', 'learning', 'ml', 'artificial', 'intelligence', 'statistical', 'algorithms', 'learn', 'data', 'explicit', 'instructions']\n",
    "\n",
    "# Calculate relevance scores\n",
    "relevance_scores = calculate_relevance_scores(true_keywords, predicted_keywords_with_scores)\n",
    "\n",
    "# Evaluate keyword extraction\n",
    "mean_ndcg, mean_mrr = evaluate_keyword_extraction([true_keywords], [predicted_keywords_with_scores])\n",
    "\n",
    "print(\"Mean NDCG:\", mean_ndcg)\n",
    "print(\"Mean MRR:\", mean_mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
