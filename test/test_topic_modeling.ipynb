{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\python310\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in c:\\python310\\lib\\site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from datasets) (1.24.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\python310\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\python310\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\python310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\python310\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\python310\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\python310\\lib\\site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\python310\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python310\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of processed data:\n",
      "Tokens: ['here', ',', 'at', 'a', 'glance', ',', 'are', 'developments', 'today', 'involving']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"midas/duc2001\", \"raw\")[\"test\"]\n",
    "\n",
    "def preprocess_dataset_direct_use(dataset):\n",
    "    processed_data = []\n",
    "\n",
    "    for item in dataset:\n",
    "        # Directly use tokens and BIO tags from the dataset\n",
    "        # but only apply lower to token\n",
    "        tokens = [token.lower() for token in item['document']]\n",
    "        bio_tags = item['doc_bio_tags'] \n",
    "        \n",
    "        processed_data.append({'tokens': tokens, 'labels': bio_tags})\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Assuming direct compatibility\n",
    "processed_dataset = preprocess_dataset_direct_use(dataset)\n",
    "\n",
    "# Display a sample of the processed data\n",
    "print(\"Sample of processed data:\")\n",
    "for data in processed_dataset[:1]:  # Displaying the first sample\n",
    "    print(\"Tokens:\", data['tokens'][:10])\n",
    "    print(\"Labels:\", data['labels'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_pretrained_bert in c:\\python310\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (1.24.2)\n",
      "Requirement already satisfied: boto3 in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (1.34.82)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (4.66.1)\n",
      "Requirement already satisfied: regex in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2024.2.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.82 in c:\\python310\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (1.34.82)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\python310\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\python310\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->pytorch_pretrained_bert) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from botocore<1.35.0,>=1.34.82->boto3->pytorch_pretrained_bert) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python310\\lib\\site-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.82->boto3->pytorch_pretrained_bert) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "processed_dataset = preprocess_dataset_direct_use(dataset)  # This assumes the function is defined as before\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Extract tokens and labels\n",
    "tokens = [data['tokens'] for data in processed_dataset]\n",
    "labels = [data['labels'] for data in processed_dataset]\n",
    "\n",
    "# Map labels into integers\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "tags_vals = ['B', 'I', 'O']\n",
    "\n",
    "# Convert tokens to BERT input IDs and attention masks, and labels to indices\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokens]\n",
    "input_ids = pad_sequences(input_ids, maxlen=75, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "tags = [[tag2idx.get(l) for l in lab] for lab in labels]\n",
    "tags = pad_sequences(tags, maxlen=75, value=tag2idx[\"O\"], padding=\"post\", dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "# Ensure labels are long type\n",
    "tr_inputs = torch.tensor(tr_inputs, dtype=torch.long)\n",
    "val_inputs = torch.tensor(val_inputs, dtype=torch.long)\n",
    "tr_tags = torch.tensor(tr_tags, dtype=torch.long)  \n",
    "val_tags = torch.tensor(val_tags, dtype=torch.long)\n",
    "tr_masks = torch.tensor(tr_masks, dtype=torch.long)\n",
    "val_masks = torch.tensor(val_masks, dtype=torch.long)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the processed_dataset is already defined and loaded as before\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(tag2idx),  # The number of output labels. 2 for binary classification.\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Setting custom optimization parameters.\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,  # Default value\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Training Loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [01:58<05:55, 118.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n",
      "\n",
      "Average Training Loss: 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [03:49<03:47, 114.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n",
      "\n",
      "Average Training Loss: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [05:38<01:51, 111.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n",
      "\n",
      "Average Training Loss: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [07:36<00:00, 114.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Training loop\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    print(f'\\nAverage Training Loss: {avg_train_loss:.2f}')\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure the model's performance on our validation set.\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    print(f'Validation Accuracy: {eval_accuracy/nb_eval_steps:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8963440860215054\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the tokenizer\n",
    "model.save_pretrained('./model_save/')\n",
    "tokenizer.save_pretrained('./model_save/')\n",
    "\n",
    "# Load the model and the tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./model_save/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./model_save/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: []\n"
     ]
    }
   ],
   "source": [
    "def keywordextract(sentence, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    tkns = tokenizer.tokenize(sentence)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tkns)\n",
    "    # Prepare for model input\n",
    "    input_ids = torch.tensor([indexed_tokens]).to(device)\n",
    "    attention_mask = torch.tensor([[1]*len(indexed_tokens)]).to(device)  # Assuming all tokens are not padding\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    keywords = []\n",
    "    for token_id, prediction_idx in zip(input_ids[0], predictions[0]):\n",
    "        # Extracting tokens classified as 'B' or 'I'\n",
    "        if prediction_idx.item() in [tag2idx['B'], tag2idx['I']]:\n",
    "            keywords.append(tokenizer.convert_ids_to_tokens([token_id.item()])[0])\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# Example use\n",
    "text = \"The solution is based upon an abstract representation of the mobile object system.\"\n",
    "extracted_keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", extracted_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: []\n"
     ]
    }
   ],
   "source": [
    "def keyword_extract(sentence):\n",
    "    # Tokenize the input sentence and create the attention mask\n",
    "    inputs = tokenizer.encode_plus(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to list of predicted tag indices\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Decode the ids to tokens and tags\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "    tag_indices = predictions[0].tolist()\n",
    "    \n",
    "    # Extract keywords based on the tag indices\n",
    "    keywords = [tokens[i] for i, tag_idx in enumerate(tag_indices) if tag_idx in [tag2idx['B'], tag2idx['I']]]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# Example usage\n",
    "text = \"The solution is based upon an abstract representation of the mobile object system.\"\n",
    "keywords = keyword_extract(text)\n",
    "print(\"Extracted Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions: tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "Token-wise predictions: [('wine', 2), ('is', 2), ('an', 2), ('alcoholic', 2), ('drink', 2), ('made', 2), ('from', 2), ('fe', 2), ('##rm', 2), ('##ented', 2), ('fruit', 2), ('.', 2), ('yeast', 2), ('consume', 2), ('##s', 2), ('the', 2), ('sugar', 2), ('in', 2), ('the', 2), ('fruits', 2), ('and', 2), ('converts', 2), ('it', 2), ('to', 2), ('ethanol', 2), ('and', 2), ('carbon', 2), ('dioxide', 2), (',', 2), ('releasing', 2), ('heat', 2), ('in', 2), ('the', 2), ('process', 2), ('.', 2), ('though', 2), ('wine', 2), ('can', 2), ('be', 2), ('made', 2), ('from', 2), ('a', 2), ('variety', 2), ('of', 2), ('fruit', 2), ('crops', 2), ('such', 2), ('as', 2), ('plum', 2), (',', 2), ('cherry', 2), (',', 2), ('po', 2), ('##me', 2), ('##gra', 2), ('##nate', 2), (',', 2), ('blue', 2), ('##berry', 2), (',', 2), ('curran', 2), ('##t', 2), ('and', 2), ('elder', 2), ('##berry', 2), (',', 2), ('it', 2), ('is', 2), ('most', 2), ('often', 2), ('made', 2), ('from', 2), ('grapes', 2), (',', 2), ('and', 2), ('the', 2), ('term', 2), ('wine', 2), ('generally', 2), ('refers', 2), ('to', 2), ('grape', 2), ('wine', 2), ('when', 2), ('used', 2), ('without', 2), ('a', 2), ('qualifier', 2), ('.', 2)]\n",
      "Extracted Keywords: []\n"
     ]
    }
   ],
   "source": [
    "def keywordextract(sentence, model, tokenizer, device):\n",
    "    tkns = tokenizer.tokenize(sentence)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tkns)\n",
    "    input_ids = torch.tensor([indexed_tokens]).to(device)\n",
    "    attention_mask = torch.tensor([[1] * len(indexed_tokens)]).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    # Debug: Print raw predictions for inspection\n",
    "    print(\"Raw predictions:\", predictions)\n",
    "    print(\"Token-wise predictions:\", list(zip(tkns, predictions[0].tolist())))\n",
    "\n",
    "    keywords = [tkns[i] for i, label_idx in enumerate(predictions[0]) if label_idx in [tag2idx['B'], tag2idx['I']]]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# Assuming model, tokenizer, device, and tag2idx are already defined\n",
    "text = \"Wine is an alcoholic drink made from fermented fruit. Yeast consumes the sugar in the fruits and converts it to ethanol and carbon dioxide, releasing heat in the process. Though wine can be made from a variety of fruit crops such as plum, cherry, pomegranate, blueberry, currant and elderberry, it is most often made from grapes, and the term wine generally refers to grape wine when used without a qualifier.\"\n",
    "extracted_keywords = keywordextract(text, model, tokenizer, device)\n",
    "print(\"Extracted Keywords:\", extracted_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and Predicted Tags:\n",
      "[CLS]: 2\n",
      "wine: 2\n",
      "is: 2\n",
      "an: 2\n",
      "alcoholic: 2\n",
      "drink: 2\n",
      "made: 2\n",
      "from: 2\n",
      "fe: 2\n",
      "##rm: 2\n",
      "##ented: 2\n",
      "fruit: 2\n",
      ".: 2\n",
      "yeast: 2\n",
      "consume: 2\n",
      "##s: 2\n",
      "the: 2\n",
      "sugar: 2\n",
      "in: 2\n",
      "the: 2\n",
      "fruits: 2\n",
      "and: 2\n",
      "converts: 2\n",
      "it: 2\n",
      "to: 2\n",
      "ethanol: 2\n",
      "and: 2\n",
      "carbon: 2\n",
      "dioxide: 2\n",
      ",: 2\n",
      "releasing: 2\n",
      "heat: 2\n",
      "in: 2\n",
      "the: 2\n",
      "process: 2\n",
      ".: 2\n",
      "though: 2\n",
      "wine: 2\n",
      "can: 2\n",
      "be: 2\n",
      "made: 2\n",
      "from: 2\n",
      "a: 2\n",
      "variety: 2\n",
      "of: 2\n",
      "fruit: 2\n",
      "crops: 2\n",
      "such: 2\n",
      "as: 2\n",
      "plum: 2\n",
      ",: 2\n",
      "cherry: 2\n",
      ",: 2\n",
      "po: 2\n",
      "##me: 2\n",
      "##gra: 2\n",
      "##nate: 2\n",
      ",: 2\n",
      "blue: 2\n",
      "##berry: 2\n",
      ",: 2\n",
      "curran: 2\n",
      "##t: 2\n",
      "and: 2\n",
      "elder: 2\n",
      "##berry: 2\n",
      ",: 2\n",
      "it: 2\n",
      "is: 2\n",
      "most: 2\n",
      "often: 2\n",
      "made: 2\n",
      "from: 2\n",
      "grapes: 2\n",
      ",: 2\n",
      "and: 2\n",
      "the: 2\n",
      "term: 2\n",
      "wine: 2\n",
      "generally: 2\n",
      "refers: 2\n",
      "to: 2\n",
      "grape: 2\n",
      "wine: 2\n",
      "when: 2\n",
      "used: 2\n",
      "without: 2\n",
      "a: 2\n",
      "qualifier: 2\n",
      ".: 2\n",
      "[SEP]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "[PAD]: 2\n",
      "Extracted Keywords: []\n"
     ]
    }
   ],
   "source": [
    "def keyword_extract(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "    tag_indices = predictions[0].tolist()\n",
    "\n",
    "    # Debug: Print raw predictions and their corresponding tokens\n",
    "    print(\"Tokens and Predicted Tags:\")\n",
    "    for token, tag_idx in zip(tokens, tag_indices):\n",
    "        print(f\"{token}: {tag_idx}\")\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = [tokens[i] for i, tag_idx in enumerate(tag_indices) if tag_idx != tag2idx['O']]\n",
    "    return keywords\n",
    "\n",
    "text = \"Wine is an alcoholic drink made from fermented fruit. Yeast consumes the sugar in the fruits and converts it to ethanol and carbon dioxide, releasing heat in the process. Though wine can be made from a variety of fruit crops such as plum, cherry, pomegranate, blueberry, currant and elderberry, it is most often made from grapes, and the term wine generally refers to grape wine when used without a qualifier.\"\n",
    "keywords = keyword_extract(text)\n",
    "print(\"Extracted Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
