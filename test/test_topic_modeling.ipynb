{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from datasets) (1.24.2)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\python310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (23.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.19.4->datasets)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl (365 kB)\n",
      "   ---------------------------------------- 0.0/365.2 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 307.2/365.2 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 365.2/365.2 kB 7.6 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Downloading pyarrow-15.0.2-cp310-cp310-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.8 MB 9.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.7/24.8 MB 10.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.1/24.8 MB 8.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.5/24.8 MB 8.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.9/24.8 MB 8.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.1/24.8 MB 8.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.5/24.8 MB 8.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 3.0/24.8 MB 8.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 8.0 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.7/24.8 MB 7.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.0/24.8 MB 7.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.1/24.8 MB 7.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.3/24.8 MB 7.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.4/24.8 MB 6.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.5/24.8 MB 6.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.5/24.8 MB 6.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.5/24.8 MB 6.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.8/24.8 MB 5.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.2/24.8 MB 5.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.4/24.8 MB 5.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.8/24.8 MB 5.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.2/24.8 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.5/24.8 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.8/24.8 MB 6.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.2/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.5/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.8/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.2/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.6/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.8/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.1/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.5/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.8/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.1/24.8 MB 6.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.4/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.7/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.0/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.2/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.5/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.9/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.3/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.6/24.8 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.9/24.8 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.1/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.4/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.8/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.1/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.4/24.8 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.8/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.1/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.4/24.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.8/24.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.1/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.4/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.7/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.0/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.4/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.7/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.0/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.3/24.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.7/24.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.0/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.4/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.7/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.8/24.8 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.0/24.8 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.1/24.8 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.3/24.8 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.5/24.8 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.6/24.8 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.8/24.8 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.0/24.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.1/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.3/24.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.6/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.9/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.2/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.6/24.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "   ---------------------------------------- 0.0/145.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 145.3/145.3 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp310-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 184.3/269.5 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.5/269.5 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB 7.8 MB/s eta 0:00:00\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, typing-extensions, safetensors, pyyaml, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, filelock, dill, attrs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 datasets-2.18.0 dill-0.3.8 filelock-3.13.4 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3 typing-extensions-4.11.0 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'c:\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"midas/duc2001\", \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'doc_bio_tags', 'extractive_keyphrases', 'abstractive_keyphrases', 'other_metadata'],\n",
       "        num_rows: 308\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sentences or labels were processed.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from abbreviation import limits\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"midas/duc2001\", \"raw\")\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=limits):\n",
    "    \"\"\"Expand contractions in the text.\"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                               if contraction_mapping.get(match)\\\n",
    "                               else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Perform both contraction expansion and basic text cleaning.\"\"\"\n",
    "    text = expand_contractions(text)\n",
    "    # Additional cleaning steps as before\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_duc2001_dataset(data):\n",
    "    processed_sentences = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for item in data:\n",
    "        document = item.get('document', '')\n",
    "        document = clean_text(document)\n",
    "        keyphrases = item.get('abstractive_keyphrases', [])\n",
    "        \n",
    "        # Tokenize the document into sentences\n",
    "        sentences = sent_tokenize(document)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            bert_tokens = tokenizer.tokenize(sent)\n",
    "            bert_tokens_lower = [token.lower() for token in bert_tokens]\n",
    "            token_labels = ['O'] * len(bert_tokens)\n",
    "            \n",
    "            for kp in keyphrases:\n",
    "                kp_tokens = kp.lower().split()\n",
    "                kp_len = len(kp_tokens)\n",
    "                \n",
    "                for i in range(len(bert_tokens_lower)):\n",
    "                    if bert_tokens_lower[i:i+kp_len] == kp_tokens:\n",
    "                        token_labels[i] = 'B'  # Mark the beginning of a keyphrase\n",
    "                        for j in range(1, kp_len):\n",
    "                            if (i + j) < len(token_labels):\n",
    "                                token_labels[i + j] = 'I'  # Mark inside a keyphrase\n",
    "            \n",
    "            processed_sentences.append(bert_tokens)\n",
    "            processed_labels.append(token_labels)\n",
    "\n",
    "    return processed_sentences, processed_labels\n",
    "\n",
    "# Assuming 'dataset' is loaded as before\n",
    "sentences, labels = preprocess_duc2001_dataset(dataset['test'])\n",
    "\n",
    "if sentences and labels:  # Only proceed if both lists are non-empty\n",
    "    print(sentences[0])  # Example processed sentence (tokens)\n",
    "    print(labels[0])     # Corresponding labels\n",
    "else:\n",
    "    print(\"No sentences or labels were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sentences or labels were processed.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import BertTokenizer\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"midas/duc2001\", \"raw\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Return empty string if text is not a string\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_documents(dataframe):\n",
    "    processed_sentences = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for _, row in dataframe.iterrows():\n",
    "        document = clean_text(row['document'])\n",
    "        keyphrases = row['abstractive_keyphrases']\n",
    "        \n",
    "        sentences = sent_tokenize(document)\n",
    "        for sent in sentences:\n",
    "            tokens = tokenizer.tokenize(sent)\n",
    "            token_labels = ['O'] * len(tokens)\n",
    "            \n",
    "            for word in word_tokenize(sent):\n",
    "                is_keyword = any(word.lower() in kp.lower().split() for kp in keyphrases)\n",
    "                if is_keyword:\n",
    "                    idx = tokens.index(word) if word in tokens else -1\n",
    "                    if idx != -1:  # Found the word in tokens\n",
    "                        token_labels[idx] = 'B'\n",
    "                        # Extend to 'I' tags if the keyword is more than one token long\n",
    "                        for k in range(1, len(tokenizer.tokenize(word))):\n",
    "                            if idx + k < len(token_labels):\n",
    "                                token_labels[idx + k] = 'I'\n",
    "            \n",
    "            if set(token_labels) != {'O'}:  # Ensure we have keywords in the sentence\n",
    "                processed_sentences.append(tokens)\n",
    "                processed_labels.append(token_labels)\n",
    "\n",
    "    return processed_sentences, processed_labels\n",
    "\n",
    "# Preprocess the dataset\n",
    "sentences, labels = preprocess_documents(df)\n",
    "\n",
    "if sentences and labels:\n",
    "    for i in range(min(5, len(sentences))):  # Print first 5 or fewer processed sentences\n",
    "        print(f\"Sentence {i+1}: {sentences[i]}\")\n",
    "        print(f\"Labels {i+1}: {labels[i]}\\n\")\n",
    "else:\n",
    "    print(\"No sentences or labels were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', ',', 'at', 'a', 'glance', ',', 'are', 'developments', 'today', 'involving', 'the', 'crash', 'of', 'Pan', 'American', 'World', 'Airways', 'Flight', '103', 'Wednesday', 'night', 'in', 'Lockerbie', ',', 'Scotland', ',', 'that', 'killed', 'all', '259', 'people', 'aboard', 'and', 'more', 'than', '20', 'people', 'on', 'the', 'ground', ':']\n",
      "['terrorist threats', 'widespread wreckage', 'radical palestinian faction', 'terrorist bombing', 'bomb threat', 'sabotage']\n"
     ]
    }
   ],
   "source": [
    "first_row = datasets[\"test\"][0]\n",
    "print(first_row[\"document\"])  # This will print the 'document' content of the first row.\n",
    "print(first_row[\"abstractive_keyphrases\"])  # Similarly for 'abstractive_keyphrases'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
