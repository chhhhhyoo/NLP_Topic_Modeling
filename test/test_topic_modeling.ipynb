{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch_pretrained_bert --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of processed data:\n",
      "Tokens: ['here', ',', 'at', 'a', 'glance', ',', 'are', 'developments', 'today', 'involving']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"midas/duc2001\", \"raw\")[\"test\"]\n",
    "\n",
    "def preprocess_dataset_direct_use(dataset):\n",
    "    processed_data = []\n",
    "\n",
    "    for item in dataset:\n",
    "        # Directly use tokens and BIO tags from the dataset\n",
    "        # apply lower to token\n",
    "        tokens = [token.lower() for token in item['document']]\n",
    "        bio_tags = item['doc_bio_tags'] \n",
    "        \n",
    "        processed_data.append({'tokens': tokens, 'labels': bio_tags})\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "processed_dataset = preprocess_dataset_direct_use(dataset)\n",
    "\n",
    "# Display a sample of the processed data\n",
    "print(\"Sample of processed data:\")\n",
    "for data in processed_dataset[:1]:  # Displaying the first sample\n",
    "    print(\"Tokens:\", data['tokens'][:10])\n",
    "    print(\"Labels:\", data['labels'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Extract tokens and labels\n",
    "tokens = [data['tokens'] for data in processed_dataset]\n",
    "labels = [data['labels'] for data in processed_dataset]\n",
    "\n",
    "# Map labels into integers\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "tags_vals = ['B', 'I', 'O']\n",
    "\n",
    "# Convert tokens to BERT input IDs and attention masks, and labels to indices\n",
    "# Padding value(ensure all sequences of tags uniform length) = 'O' => does not affect\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokens]\n",
    "input_ids = pad_sequences(input_ids, maxlen=75, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "tags = [[tag2idx.get(l) for l in lab] for lab in labels]\n",
    "tags = pad_sequences(tags, maxlen=75, value=tag2idx[\"O\"], padding=\"post\", dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "# to focus on the meaningful part of the input\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "# Ensure labels are long type\n",
    "tr_inputs = torch.tensor(tr_inputs, dtype=torch.long)\n",
    "val_inputs = torch.tensor(val_inputs, dtype=torch.long)\n",
    "tr_tags = torch.tensor(tr_tags, dtype=torch.long)  \n",
    "val_tags = torch.tensor(val_tags, dtype=torch.long)\n",
    "tr_masks = torch.tensor(tr_masks, dtype=torch.long)\n",
    "val_masks = torch.tensor(val_masks, dtype=torch.long)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(tag2idx),  # The number of output labels. 3 for our case of B,I,O\n",
    "    # output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    # output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Setting custom optimization parameters.\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,    \n",
    "    num_warmup_steps=0,  # Default value\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Training Loss: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [02:19<06:57, 139.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n",
      "\n",
      "Average Training Loss: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [05:23<05:31, 165.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n",
      "\n",
      "Average Training Loss: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [08:30<02:55, 175.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n",
      "\n",
      "Average Training Loss: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [11:44<00:00, 176.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Training loop\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    print(f'\\nAverage Training Loss: {avg_train_loss:.2f}')\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure the model's performance on our validation set.\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    print(f'Validation Accuracy: {eval_accuracy/nb_eval_steps:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8963440860215054\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the tokenizer\n",
    "model.save_pretrained('./model_save/')\n",
    "tokenizer.save_pretrained('./model_save/')\n",
    "\n",
    "# Load the model and the tokenizer\n",
    "model = BertForTokenClassification.from_pretrained('./model_save/')\n",
    "tokenizer = BertTokenizer.from_pretrained('./model_save/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordextract(sentence, model, tokenizer, device):\n",
    "    # Tokenize input\n",
    "    tokens = tokenizer.encode_plus(sentence, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    input_ids = tokens['input_ids'].to(device)\n",
    "    attention_mask = tokens['attention_mask'].to(device)\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).squeeze().tolist()  # Convert to list for easier processing\n",
    "\n",
    "    # Display tokens and their corresponding predicted tags\n",
    "    tokenized_sentence = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    print(\"Tokens and their predicted tags:\")\n",
    "    for token, prediction in zip(tokenized_sentence, predictions):\n",
    "        print(f\"{token}: {tags_vals[prediction]}\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and their predicted tags:\n",
      "[CLS]: O\n",
      "the: O\n",
      "articles: O\n",
      "have: O\n",
      "been: O\n",
      "grouped: O\n",
      "into: O\n",
      "30: O\n",
      "clusters: O\n",
      "manually: O\n",
      "by: O\n",
      "ni: O\n",
      "##st: O\n",
      "ann: O\n",
      "##ota: O\n",
      "##tors: O\n",
      "for: O\n",
      "multi: O\n",
      "##do: O\n",
      "##cum: O\n",
      "##ent: O\n",
      "sum: O\n",
      "##mar: O\n",
      "##ization: O\n",
      ",: O\n",
      "and: O\n",
      "the: O\n",
      "documents: O\n",
      "within: O\n",
      "each: O\n",
      "cluster: O\n",
      "were: O\n",
      "topic: O\n",
      "-: O\n",
      "related: O\n",
      "or: O\n",
      "relevant: O\n",
      ".: O\n",
      "the: O\n",
      "manually: O\n",
      "labeled: O\n",
      "clusters: O\n",
      "were: O\n",
      "considered: O\n",
      "as: O\n",
      "the: O\n",
      "ground: O\n",
      "truth: O\n",
      "clusters: O\n",
      "or: O\n",
      "gold: O\n",
      "clusters: O\n",
      ".: O\n",
      "in: O\n",
      "order: O\n",
      "to: O\n",
      "investigate: O\n",
      "existing: O\n",
      "cluster: O\n",
      "##ing: O\n",
      "algorithms: O\n",
      ",: O\n",
      "the: O\n",
      "documents: O\n",
      "in: O\n",
      "the: O\n",
      "clusters: O\n",
      "were: O\n",
      "mixed: O\n",
      "together: O\n",
      "to: O\n",
      "form: O\n",
      "the: O\n",
      "whole: O\n",
      "document: O\n",
      "set: O\n",
      "for: O\n",
      "automatic: O\n",
      "cluster: O\n",
      "##ing: O\n",
      ".: O\n",
      "[SEP]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[PAD]: O\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The articles have been grouped into 30 clusters \n",
    "manually by NIST annotators for multidocument summarization, and the documents \n",
    "within each cluster were topic-related or relevant.\n",
    "The manually labeled clusters were considered as\n",
    "the ground truth clusters or gold clusters. In order\n",
    "to investigate existing clustering algorithms, the\n",
    "documents in the clusters were mixed together to\n",
    "form the whole document set for automatic clustering.\"\"\"\n",
    "print(keywordextract(text, model, tokenizer, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: Counter({'O': 253311, 'B': 4365, 'I': 3273})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of labels to get the overall distribution\n",
    "all_labels = [label for sublist in labels for label in sublist]\n",
    "\n",
    "# Count each type of label\n",
    "label_counter = Counter(all_labels)\n",
    "print(\"Label distribution:\", label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
