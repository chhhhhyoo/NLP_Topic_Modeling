{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from datasets) (1.24.2)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\python310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (23.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.19.4->datasets)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl (365 kB)\n",
      "   ---------------------------------------- 0.0/365.2 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 307.2/365.2 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 365.2/365.2 kB 7.6 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Downloading pyarrow-15.0.2-cp310-cp310-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.8 MB 9.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.7/24.8 MB 10.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.1/24.8 MB 8.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.5/24.8 MB 8.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.9/24.8 MB 8.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.1/24.8 MB 8.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.5/24.8 MB 8.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 3.0/24.8 MB 8.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 8.0 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.7/24.8 MB 7.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.0/24.8 MB 7.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.1/24.8 MB 7.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.3/24.8 MB 7.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.4/24.8 MB 6.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.5/24.8 MB 6.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.5/24.8 MB 6.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.5/24.8 MB 6.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.8/24.8 MB 5.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.2/24.8 MB 5.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.4/24.8 MB 5.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.8/24.8 MB 5.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.2/24.8 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.5/24.8 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.8/24.8 MB 6.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.2/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.5/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.8/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.2/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.6/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.8/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.1/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.5/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.8/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.1/24.8 MB 6.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.4/24.8 MB 6.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.7/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.0/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.2/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.5/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.9/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.3/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.6/24.8 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.9/24.8 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.1/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.4/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.8/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.1/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.4/24.8 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.8/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.1/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.4/24.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.8/24.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.1/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.4/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.7/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.0/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.4/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.7/24.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.0/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.3/24.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.7/24.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.0/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.4/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.7/24.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.8/24.8 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.0/24.8 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.1/24.8 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.3/24.8 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.5/24.8 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.6/24.8 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.8/24.8 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.0/24.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.1/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.3/24.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.6/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.9/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.2/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.6/24.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "   ---------------------------------------- 0.0/145.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 145.3/145.3 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp310-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 184.3/269.5 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.5/269.5 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB 7.8 MB/s eta 0:00:00\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, typing-extensions, safetensors, pyyaml, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, filelock, dill, attrs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 datasets-2.18.0 dill-0.3.8 filelock-3.13.4 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyyaml-6.0.1 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3 typing-extensions-4.11.0 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'c:\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of processed data:\n",
      "Tokens: ['here', ',', 'at', 'a', 'glance', ',', 'are', 'developments', 'today', 'involving']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"midas/duc2001\", \"raw\")[\"test\"]\n",
    "\n",
    "def preprocess_dataset_direct_use(dataset):\n",
    "    processed_data = []\n",
    "\n",
    "    for item in dataset:\n",
    "        # Directly use tokens and BIO tags from the dataset\n",
    "        # but only apply lower to token\n",
    "        tokens = [token.lower() for token in item['document']]\n",
    "        bio_tags = item['doc_bio_tags'] \n",
    "        \n",
    "        processed_data.append({'tokens': tokens, 'labels': bio_tags})\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Assuming direct compatibility\n",
    "processed_dataset = preprocess_dataset_direct_use(dataset)\n",
    "\n",
    "# Display a sample of the processed data\n",
    "print(\"Sample of processed data:\")\n",
    "for data in processed_dataset[:1]:  # Displaying the first sample\n",
    "    print(\"Tokens:\", data['tokens'][:10])\n",
    "    print(\"Labels:\", data['labels'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_bertNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n",
      "     ---------------------------------------- 0.0/86.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.7/86.7 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (1.24.2)\n",
      "Collecting boto3 (from pytorch_pretrained_bert)\n",
      "  Downloading boto3-1.34.82-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (4.66.1)\n",
      "Requirement already satisfied: regex in c:\\python310\\lib\\site-packages (from pytorch_pretrained_bert) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2024.2.0)\n",
      "Collecting botocore<1.35.0,>=1.34.82 (from boto3->pytorch_pretrained_bert)\n",
      "  Downloading botocore-1.34.82-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_pretrained_bert)\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->pytorch_pretrained_bert) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from botocore<1.35.0,>=1.34.82->boto3->pytorch_pretrained_bert) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python310\\lib\\site-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.82->boto3->pytorch_pretrained_bert) (1.16.0)\n",
      "Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "   ---------------------------------------- 0.0/123.8 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 92.2/123.8 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 123.8/123.8 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading boto3-1.34.82-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.3/139.3 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading botocore-1.34.82-py3-none-any.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.1 MB 4.6 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.3/12.1 MB 3.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.6/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.8/12.1 MB 4.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.1/12.1 MB 4.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.1 MB 4.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.2/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.4/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.0/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.8/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.0/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.1/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.7/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.6/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.1/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.4/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.4/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.7/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.2/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.4/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.0/12.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.1 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/12.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.1 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.2/82.2 kB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert\n",
      "Successfully installed boto3-1.34.82 botocore-1.34.82 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pytorch_pretrained_bert.exe is installed in 'c:\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "processed_dataset = preprocess_dataset_direct_use(dataset)  # This assumes the function is defined as before\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Extract tokens and labels\n",
    "tokens = [data['tokens'] for data in processed_dataset]\n",
    "labels = [data['labels'] for data in processed_dataset]\n",
    "\n",
    "# Map labels into integers\n",
    "tag2idx = {'B': 0, 'I': 1, 'O': 2}\n",
    "tags_vals = ['B', 'I', 'O']\n",
    "\n",
    "# Convert tokens to BERT input IDs and attention masks, and labels to indices\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokens]\n",
    "input_ids = pad_sequences(input_ids, maxlen=75, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "tags = [[tag2idx.get(l) for l in lab] for lab in labels]\n",
    "tags = pad_sequences(tags, maxlen=75, value=tag2idx[\"O\"], padding=\"post\", dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (2.2.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp310-cp310-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.2-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python310\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python310\\lib\\site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.2/1.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.4/1.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.5/1.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.7/1.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.9/1.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.2.2-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.5/2.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.5/2.4 MB 4.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.0/2.4 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.2.2 torchvision-0.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nBertForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define the model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tag2idx),  \u001b[38;5;66;03m# The number of output labels. 2 for binary classification.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Whether the model returns attentions weights.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Whether the model returns all hidden-states.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Setting custom optimization parameters.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1412\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1412\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1400\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1398\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nBertForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the processed_dataset is already defined and loaded as before\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(tag2idx),  # The number of output labels. 2 for binary classification.\n",
    "    output_attentions=False,  # Whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Setting custom optimization parameters.\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,  # Default value\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
